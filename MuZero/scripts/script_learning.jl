using AlphaZero
using ProgressLogging, TensorBoardLogger, Logging
import Flux
import FileIO
import Random

import LinearAlgebra
LinearAlgebra.BLAS.set_num_threads(1)

include("../mu_game_wrapper.jl")
include("../network.jl")
include("../alphazerolike.jl")
include("../trace.jl")
include("../play.jl")
include("../training.jl")
include("../learning.jl")
include("../benchmark.jl")

gspec = Examples.games["tictactoe"]

n=3
self_play = (;
  sim=SimParams(
    num_games=500,
    num_workers=n,
    batch_size=n,
    use_gpu=false,
    reset_every=4, #not used, mcts resets everytime
    flip_probability=0.,
    alternate_colors=false),
  mcts = MctsParams(
    num_iters_per_turn=64,
    cpuct=2.5,
    temperature=ConstSchedule(1.0),
    dirichlet_noise_ϵ=0.25,
    dirichlet_noise_α=0.5))

arena = (;
  sim=SimParams(
    num_games=100,
    num_workers=n,
    batch_size=n,
    use_gpu=false,
    reset_every=1,
    flip_probability=0.5,
    alternate_colors=true),
  mcts = MctsParams(
    self_play.mcts,
    temperature=ConstSchedule(0.3),
    dirichlet_noise_ϵ=0.1),
  update_threshold=0.00)


learning_params = (;
  num_unroll_steps=5, #if =0, g is not learning
  td_steps=10, # with max length=9, always go till the end of the game, rootvalues don't count
  discount=0.997,
  l2_regularization=1f-4, #Float32
  # l2_regularization=0f0, #Float32
  loss_computation_batch_size=256,
  batches_per_checkpoint=1000,
  num_checkpoints=1,
  learning_rate=0.003,
  momentum=0.9)

benchmark_sim = SimParams(
    num_games=100,
    num_workers=2,
    batch_size=4,
    use_gpu=false,
    reset_every=1,
    flip_probability=0.5,
    alternate_colors=true)

benchmark = [
  Benchmark.Duel(
    Mu(self_play.mcts),
    Benchmark.MctsRollouts(self_play.mcts),
    benchmark_sim)]

  #memory = CircularBuffer{Trace{GI.state_type(gspec)}}(1024)

  μparams = MuParams(self_play, learning_params, arena, 6, 3000)
  # μparams = MuParams(self_play, learning_params, arena, 1, 5000)

  # αnns = (
  #   f=PredictionNetwork(
  #     gspec,
  #     PredictionHP(hiddenstate_shape=27, width=256, depth_common=6,
  #       use_batch_norm=true, batch_norm_momentum=1.)),
  #   g=AlphaDynamics(gspec), # set num_unroll_steps=0 to not use it during learning
  #   h=AlphaRepresentation())

  # αnns = (
  #   f=AlphaPrediction(
  #       PredictionNetwork(
  #         gspec,
  #         PredictionHP(hiddenstate_shape=27, width=256, depth_common=6,
  #           use_batch_norm=true, batch_norm_momentum=1.))),
  #   g=AlphaDynamics(gspec), # set num_unroll_steps=0 to not use it during learning
  #   h=AlphaRepresentation())
  Random.seed!(2138)
  # env = MuEnv(gspec, μparams, αnns, experience=FileIO.load("memory_minmax.jld2", "mem"))
  # env = MuEnv(gspec, μparams, αnns, experience=FileIO.load("memory_minmax_d5_t025.jld2", "mem"))

##

μNetworkHP = MuNetworkHP(
  gspec,
  PredictionHP(hiddenstate_shape=32, width=256, depth_common=5, use_batch_norm=true, batch_norm_momentum=1.),
  DynamicsHP(hiddenstate_shape=32, width=256, depth_common=1, use_batch_norm=true, batch_norm_momentum=1.),
  RepresentationHP(hiddenstate_shape=32, width=32, depth=0))
# μNetworkHP = MuNetworkHP(gspec,
#   PredictionHP(hiddenstate_shape=32, width=64, depth_common=4),
#   DynamicsHP(hiddenstate_shape=32, width=64, depth_common=4),
#   RepresentationHP(width=64, depth=4, hiddenstate_shape=32))
  env = MuEnv(gspec, μparams, MuNetwork(μNetworkHP), experience=FileIO.load("memory_minmax_d5_t025.jld2", "mem"))
  # @info "blah" memory_analysis(env.memory)...
  # @logprogress have @debug level, and I moved all losses into @debug also, 
# mean_loss is @info
tblogger=TBLogger("tensorboard_logs/run", min_level=Logging.Debug) 
  # LEARNING STEP
  with_logger(tblogger) do 
    @info "params" params=μparams
    # @info "network params" μNetworkHP 
    @info "alphalike params" αnns.f.net.hyper
    # train!(env; benchmark=benchmark) 
    @info "Memory Analysis" memory_analysis(env.memory)... "generated by MinMax player"
    @timed learning_step!(env)
  end

  # TRAIN
  with_logger(tblogger) do 
    @info "params" μparams
    @info "alphalike params" αnns.f.net.hyper
    AlphaZero.Util.@printing_errors train!(env, benchmark=benchmark)
  end
run_duel(env,benchmark)
  @enter rewards, redundancy = pit_networks(gspec, env.curnns, env.bestnns, arena)
  rewards, redundancy = pit_networks(gspec, env.curnns, env.bestnns, arena)
  mean(rewards)
# FileIO.save("env.jld2", "env", env)

  train!(env)

#generate minmax samples
  # with_logger(tblogger) do
  env = MuEnv(gspec, μparams, αnns)

    @info "params" params=μparams
    AlphaZero.Util.@printing_errors @timed self_play_step!(env)
    @info "Memory Analysis" memory_analysis(env.memory)... "generated by MinMax player τ=0.25, depth=5"
    # FileIO.save("memory_minmax_d5_t025.jld2", "mem", env.memory)
  # end

count(last(t.rewards)==(1) for t in env.memory)

  hyper = env.params.learning_params
  traces = [sample_trace(env.memory) for _ in 1:2] #? change to iterator
  trace_pos_idxs = [sample_position(t) for t in traces]
  sample = make_target(gspec, traces[1], 6, hyper)
  samples = [make_target(gspec, t, i, hyper) for (t,i) in zip(traces, trace_pos_idxs)]
  # FileIO.save("sample.jld2", "sample_st6",sample)
  sample = FileIO.load("sample.jld2", "sample_st6")
samples = [sample]
  X       = Flux.batch(smpl.x       for smpl in samples)
  A_mask  = Flux.batch(smpl.a_mask  for smpl in samples)
  As      = Flux.batch(smpl.as      for smpl in samples)
  Ps      = Flux.batch(smpl.ps      for smpl in samples)
  Vs      = Flux.batch(smpl.vs      for smpl in samples)
  Rs      = Flux.batch(smpl.rs      for smpl in samples)
  f32(arr) = convert(AbstractArray{Float32}, arr)
data =  map(x -> f32(x), (; X, A_mask, As, Ps, Vs, Rs))

(data.X, data.As)

  # data = [sample_batch(gspec, env.memory, env.params.learning_params) for _ in 1:1]
  losses(env.curnns, env.params.learning_params, data)
  @enter losses(env.curnns, env.params.learning_params, data)


  t = @timed self_play_step!(env)
  @timed learning_step!(env)
  # @enter self_play_step!(env)
  @enter learning_step!(env)

  # FileIO.save("env.jld2", env)
  mem_test = FileIO.load("memory_minmax.jld2", "mem")

  # for (b1,b2) in zip(env.memory, mem_test)
  #   @assert b1 ≂ b2
  # end
  # mem_test["mem"].buffer  env.memory.buffer
  # mem_test["mem"].buffer[1]
  # env.memory.buffer[1]       

  # learning_step
lp = env.params.learning_params
tr = MuTrainer(env.gspec, env.curnns, env.memory, env.params.learning_params, Flux.ADAM(lp.learning_rate))
nbatches = lp.batches_per_checkpoint

update_weights!(tr, nbatches)
samples = [sample_batch(tr.gspec, tr.memory, tr.hyper) for _ in 1:2]
d = samples[1]
@enter losses(tr.nns, tr.hyper, d)
@info d.X[:,:,1,1]




#TODO check with MCTS, MinMax
#Revise.jl, tensorboard_logger

# # checking Losses
# zero(Ps)
# one.(Ps)
# Flux.Losses.mse(zero(Vs), one.(Vs), agg=x->mean(∇_scale .* x)/sum(∇_scale)) 
# Flux.Losses.mse(zero(Vs), one.(Vs), agg=x->mean(x .* ∇_scale)) 
# Flux.Losses.mse(zero(Vs), one.(Vs), agg=x->sum(∇_scale .* x) / sum(∇_scale)) 
# Flux.Losses.mse(zero(Vs), one.(Vs)) 
# sum(∇_scale)
# one.(Vs)
# Vs
# one.(Ps) .* ∇_scale

# Flux.Losses.crossentropy(one.(Ps).*ℯ, one.(Ps), agg=x->mean(x,∇_scale))
# log.(one.(Ps) .* ℯ)
# log.(zero(Ps)+eps.(Ps))
# one.(Ps) .* 
# one.(Ps) .* log.(one.(Ps).*ℯ)
# mean(.-sum(one.(Ps) .* log.(one.(Ps).*ℯ), dims=1), weights(∇_scale))
# ∇_scale .* 2

"Lenient comparison operator for `struct`, both mutable and immutable (type with \\eqsim)."
@generated function ≂(x, y)
  if !isempty(fieldnames(x)) && x == y
    mapreduce(n -> :(x.$n ≂ y.$n), (a,b)->:($a && $b), fieldnames(x))
  else
    :(x == y)
  end
end

# nns = deepcopy(tr.nns)
# nns == tr.nns
# nns ≂ tr.nns
# env.curnns ≂ env.bestnns
# nns = deepcopy(env.bestnns)
# nns ≂ env.bestnns
# fieldnames(typeof(nns))
# typeof(nns) == typeof(env.bestnns)
# nns.f.common

(X, A_mask, As, Ps, Vs, Rs) = sample_batch(env.gspec, env.memory, env.params.learning_params)
@enter losses(env.curnns, learning_params, (X, A_mask, As, Ps, Vs, Rs))
As
A = As[1:1,:]
Vs


env.curnns = deepcopy(env.bestnns)
@assert env.curnns ≂ env.bestnns    # all their fields are the same
@assert !(env.curnns == env.bestnns) # they have not the same memory adress

tr = MuTrainer(env.gspec, env.curnns, env.memory, env.params.learning_params, Flux.ADAM())
@assert env.curnns ≂ tr.nns 
@assert env.curnns == tr.nns # same memory adress (shallow copy)


@timed update_weights!(tr, tr.hyper.batches_per_checkpoint)
@assert env.curnns == tr.nns
@assert !(env.bestnns ≂ tr.nns) # after update! tr.nns and env.curnns changed
@assert !(env.curnns ≂ env.bestnns)


learning_step!(env)
train!(env)


# gnerator with rand
function foo(x)
  y = x+1
  z = x+1
  return (; x,y,z)
end
rands = (rand() for _ in 1:5)
foos = (foo(r) for r in rands)
X = [f.x for f in foos] # Flux.batch(f.x for f in foos)
Y = [f.y for f in foos]
Z = [f.z for f in foos]
X[1], Y[1], Z[1]