using AlphaZero
using ProgressLogging
using TensorBoardLogger
using Logging
using ParameterSchedulers: Scheduler, Cos
import Flux
import FileIO
import Random
import CUDA

import LinearAlgebra
LinearAlgebra.BLAS.set_num_threads(1)

include("../mu_game_wrapper.jl")
include("../network.jl")
include("../alphazerolike.jl")
include("../trace.jl")
include("../play.jl")
include("../simulations.jl")
include("../training.jl")
include("../learning.jl")
include("../benchmark.jl")
include("../probe-games.jl")



gspec = Examples.games["tictactoe"]

n=16
self_play = (;
  sim=SimParams(
    num_games=100,
    num_workers=n,
    batch_size=8,
    use_gpu=false,
    # todevice=Flux.cpu, #? 
    reset_every=4, #not used, mcts resets everytime
    flip_probability=0.,
    alternate_colors=false),
  mcts = MctsParams(
    num_iters_per_turn=64, #1000 benchmark
    cpuct=1.25,
    temperature=ConstSchedule(1.0),
    dirichlet_noise_ϵ=0.25,
    dirichlet_noise_α=0.1),
    device=Flux.cpu)

arena = (;
  sim=SimParams(
    num_games=50,
    num_workers=n,
    batch_size=n÷2,
    use_gpu=false,
    reset_every=1,
    flip_probability=0., #0.5
    alternate_colors=true),
  mcts = MctsParams(
    self_play.mcts,
    temperature=ConstSchedule(0.3),
    dirichlet_noise_ϵ=0.1),
  update_threshold=0.0,
  device=Flux.cpu)


learning_params = (;
  num_unroll_steps=5, #if =0, g is not learning, muzero-general=20
  td_steps=20, # with max length=9, always go till the end of the game, rootvalues don't count
  discount=0.997,
  #// value_loss_weight = 0.25, #TODO
  l2_regularization=1f-4, #Float32
  #// l2_regularization=0f0, #Float32
  loss_computation_batch_size=512,
  batches_per_checkpoint=10,
  num_checkpoints=1,
  opt=Scheduler(
    Cos(λ0=1e-3, λ1=1e-5, period=10000), # cosine annealing, google 2e4, generat doesn't use any
    Flux.ADAM()
  ),
  device=Flux.cpu
)

benchmark_sim = SimParams(
    num_games=400,
    num_workers=n,
    batch_size=n÷2,
    use_gpu=false,
    reset_every=1,
    flip_probability=0.0, #0.5 
    alternate_colors=true)

bench_mcts = MctsParams(
  num_iters_per_turn=400, #1000 benchmark
  cpuct=1.25,
  temperature=ConstSchedule(0.3),
  dirichlet_noise_ϵ=0.25,
  dirichlet_noise_α=0.1)

benchmark = (;
  vanilla_mcts=Benchmark.Duel(
    Mu(arena.mcts),
    Benchmark.MctsRollouts(bench_mcts),
    benchmark_sim),
  minmax_d5=Benchmark.Duel(
    Mu(arena.mcts),
    Benchmark.MinMaxTS(depth=5, amplify_rewards=true, τ=1.),
    benchmark_sim)
)


  μparams = MuParams(self_play, learning_params, arena, 4_000, 3000)

  Random.seed!(2137)

# muzero-general params
μNetworkHP = MuNetworkHP(
  gspec,
  PredictionHP(hiddenstate_shape=32, width=64, depth_common=-1,
    depth_vectorhead=0, depth_scalarhead=0, use_batch_norm=false, batch_norm_momentum=1.),
  DynamicsHP(hiddenstate_shape=32, width=64, depth_common=-1,
   depth_vectorhead=0, depth_scalarhead=0,  use_batch_norm=false, batch_norm_momentum=1.),
  RepresentationHP(hiddenstate_shape=32, width=0, depth=-1))

  env = MuEnv(gspec, μparams, MuNetwork(μNetworkHP))

##

# mean_loss is @info

path = "results/tictactoe/" * format(now(),"yyyy-mm-ddTHHMM") * "_test_batchifier/"
tblogger=TBLogger(path, min_level=Logging.Info)
  # TRAIN
  with_logger(tblogger) do
    @info "params" params=μparams
    # @info "alphalike params" αnns.f.net.hyper
    # @timed learning_step!(env)
    @info "network params" μNetworkHP 
    # @info "Memory Analysis" memory_analysis(env.memory)... "generated by MinMax player"
    @info "Benchmark" run_duel(env, benchmark)...
    mutrain!(env, benchmark=benchmark, path=path)
  end
